{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec8ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17d992a1",
   "metadata": {},
   "source": [
    "# ğŸ“˜ Machine Learning Algorithms â€“ Complete Interview Notes\n",
    "\n",
    "> **Interview-ready ML notes with REAL textbook-style math **\n",
    "> ğŸ¯ Interview-focused | Bangla Explanation | Math + Intuition | Mini Projects  \n",
    "> âœï¸ Prepared for ML / Data Science / AI interviews\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ Contents\n",
    "1. Linear Regression  \n",
    "2. Logistic Regression  \n",
    "3. Decision Tree  \n",
    "4. Random Forest  \n",
    "5. K-Nearest Neighbors (KNN)  \n",
    "6. Support Vector Machine (SVM)  \n",
    "7. Naive Bayes  \n",
    "8. K-Means Clustering  \n",
    "9. PCA (Principal Component Analysis)  \n",
    "10. Gradient Boosting / XGBoost  \n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ Linear Regression\n",
    "\n",
    "### ğŸ”¹ Problem Type\n",
    "\n",
    "* **Regression** (continuous value prediction)\n",
    "* Example: House price, salary prediction\n",
    "\n",
    "### ğŸ”¹ Hypothesis Function\n",
    "\n",
    "$$\n",
    "\\hat{y} = wx + b\n",
    "$$\n",
    "\n",
    "* $w$ = weight (slope)\n",
    "* $b$ = bias (intercept)\n",
    "\n",
    "### ğŸ”¹ Cost Function (MSE)\n",
    "\n",
    "$$\n",
    "J(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m}(\\hat{y}_i - y_i)^2\n",
    "$$\n",
    "\n",
    "ğŸ‘‰ à¦²à¦•à§à¦·à§à¦¯: error minimize à¦•à¦°à¦¾\n",
    "\n",
    "### ğŸ”¹ Gradient Descent Update\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y}_i - y_i)x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y}_i - y_i)\n",
    "$$\n",
    "\n",
    "### ğŸ”¹ When to use\n",
    "\n",
    "* Linear relationship\n",
    "* No complex patterns\n",
    "\n",
    "### âŒ When NOT to Use\n",
    "- Non-linear data\n",
    "- High multicollinearity\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§ª Mini Project\n",
    "**House Price Prediction**  \n",
    "Features: area, rooms  \n",
    "Target: price\n",
    "\n",
    "### ğŸ”¹ Interview Trap\n",
    "\n",
    "â“ *Why divide by 2m?*\n",
    "âœ”ï¸ Derivative à¦¸à¦¹à¦œ à¦•à¦°à¦¾à¦° à¦œà¦¨à§à¦¯\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ Logistic Regression\n",
    "\n",
    "### ğŸ”¹ Problem Type\n",
    "\n",
    "* **Binary Classification**\n",
    "* Example: Spam / Not spam\n",
    "\n",
    "### ğŸ”¹ Sigmoid Function\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "### ğŸ”¹ Model Equation\n",
    "\n",
    "$$\n",
    "P(y=1|x) = \\sigma(w^Tx + b)\n",
    "$$\n",
    "\n",
    "### ğŸ”¹ Cost Function (Log Loss)\n",
    "\n",
    "$$\n",
    "J(w) = -\\frac{1}{m}\\sum_{i=1}^{m}[y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]\n",
    "$$\n",
    "\n",
    "### ğŸ”¹ Why not MSE?\n",
    "\n",
    "* Non-convex problem\n",
    "* Slow convergence\n",
    "\n",
    "\n",
    "### ğŸ§  Intuition\n",
    "Linear model â†’ sigmoid function â†’ probability â†’ decision boundary\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… When to Use\n",
    "- Binary classification\n",
    "- Small to medium dataset\n",
    "\n",
    "### âŒ When NOT to Use\n",
    "- Complex non-linear boundary\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§ª Mini Project\n",
    "**Customer Churn Prediction**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Interview Trap\n",
    "\n",
    "â“ *Is Logistic Regression linear?*\n",
    "âœ”ï¸ Linear in parameters, nonlinear in output\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Support Vector Machine (SVM)\n",
    "\n",
    "### ğŸ”¹ Idea\n",
    "\n",
    "* **Maximum margin classifier**\n",
    "\n",
    "### ğŸ”¹ Decision Boundary\n",
    "\n",
    "$$\n",
    "w^Tx + b = 0\n",
    "$$\n",
    "\n",
    "### ğŸ”¹ Optimization Objective\n",
    "\n",
    "$$\n",
    "\\min \\frac{1}{2}||w||^2\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "$$\n",
    "y_i(w^Tx_i + b) \\ge 1\n",
    "$$\n",
    "\n",
    "### ğŸ”¹ Kernel Trick\n",
    "\n",
    "$$\n",
    "K(x_i,x_j) = \\phi(x_i)^T\\phi(x_j)\n",
    "$$\n",
    "\n",
    "### ğŸ”¹ When to use\n",
    "\n",
    "* High-dimensional data\n",
    "* Small to medium dataset\n",
    "- Text classification\n",
    "\n",
    "### âŒ When NOT to Use\n",
    "- Very large dataset\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§ª Mini Project\n",
    "**Spam Email Detection**\n",
    "\n",
    "### ğŸ”¹ Interview Trap\n",
    "\n",
    "â“ *Why SVM powerful?*\n",
    "âœ”ï¸ Margin maximization + kernel trick\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ K-Nearest Neighbors (KNN)\n",
    "\n",
    "### ğŸ”¹ Idea\n",
    "\n",
    "* Distance-based learning\n",
    "\n",
    "### ğŸ”¹ Euclidean Distance\n",
    "\n",
    "$$\n",
    "d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}\n",
    "$$\n",
    "\n",
    "### ğŸ”¹ Characteristics\n",
    "\n",
    "* No training phase\n",
    "* High memory usage\n",
    "\n",
    "### ğŸ”¹ When to use\n",
    "\n",
    "* Small dataset\n",
    "* Simple patterns\n",
    "\n",
    "### ğŸ”¹ Interview Trap\n",
    "\n",
    "â“ *Why KNN slow?*\n",
    "âœ”ï¸ Every prediction needs full dataset\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£ Naive Bayes\n",
    "\n",
    "### ğŸ”¹ Bayes Theorem\n",
    "\n",
    "$$\n",
    "P(y|x) = \\frac{P(x|y)P(y)}{P(x)}\n",
    "$$\n",
    "\n",
    "### ğŸ”¹ Naive Assumption\n",
    "\n",
    "$$\n",
    "P(x_1,x_2,...,x_n|y) = \\prod_{i=1}^{n}P(x_i|y)\n",
    "$$\n",
    "\n",
    "### ğŸ”¹ When to use\n",
    "\n",
    "* Text classification\n",
    "* Spam filtering\n",
    "- Fast baseline model\n",
    "\n",
    "### âŒ When NOT to Use\n",
    "- Feature dependency à¦¬à§‡à¦¶à¦¿ à¦¹à¦²à§‡\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§ª Mini Project\n",
    "**SMS Spam Classifier**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Interview Trap\n",
    "\n",
    "â“ *Why works despite wrong assumption?*\n",
    "âœ”ï¸ Probability dominance\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£ Decision Tree\n",
    "\n",
    "### ğŸ”¹ Entropy\n",
    "\n",
    "$$\n",
    "Entropy = -\\sum p_i\\log_2 p_i\n",
    "$$\n",
    "\n",
    "### ğŸ”¹ Information Gain\n",
    "\n",
    "$$\n",
    "IG = Entropy(parent) - \\sum \\frac{n_i}{n}Entropy(child)\n",
    "$$\n",
    "\n",
    "### ğŸ”¹ Pros\n",
    "\n",
    "* Explainable\n",
    "* No scaling needed\n",
    "\n",
    "### ğŸ”¹ Cons\n",
    "\n",
    "* Overfitting\n",
    "\n",
    "\n",
    "### âœ… When to Use\n",
    "- Explainability à¦¦à¦°à¦•à¦¾à¦°\n",
    "- Categorical + numerical data\n",
    "\n",
    "### âŒ When NOT to Use\n",
    "- Overfitting à¦¹à¦²à§‡\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§ª Mini Project\n",
    "**Loan Approval Prediction**\n",
    "\n",
    "---\n",
    "\n",
    "## 7ï¸âƒ£ Random Forest\n",
    "\n",
    "### ğŸ”¹ Idea\n",
    "\n",
    "* Ensemble of Decision Trees\n",
    "\n",
    "### ğŸ”¹ Bagging\n",
    "\n",
    "$$\n",
    "Final\\ Prediction = Majority\\ Vote\n",
    "$$\n",
    "\n",
    "### ğŸ”¹ Advantage\n",
    "\n",
    "* Reduces variance\n",
    "\n",
    "### ğŸ”¹ Interview Trap\n",
    "\n",
    "â“ *Why better than single tree?*\n",
    "âœ”ï¸ Law of large numbers\n",
    "\n",
    "### âœ… When to Use\n",
    "- High accuracy à¦¦à¦°à¦•à¦¾à¦°\n",
    "- Overfitting à¦•à¦®à¦¾à¦¤à§‡\n",
    "\n",
    "### âŒ When NOT to Use\n",
    "- Model explain à¦•à¦°à¦¾ à¦¦à¦°à¦•à¦¾à¦° à¦¹à¦²à§‡\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§ª Mini Project\n",
    "**Credit Card Fraud Detection**\n",
    "\n",
    "---\n",
    "\n",
    "## 8ï¸âƒ£ K-Means Clustering\n",
    "\n",
    "### ğŸ”¹ Objective Function\n",
    "\n",
    "$$\n",
    "J = \\sum_{i=1}^{k}\\sum_{x \\in C_i}||x - \\mu_i||^2\n",
    "$$\n",
    "\n",
    "### ğŸ”¹ Steps\n",
    "\n",
    "1. Initialize centroids\n",
    "2. Assign points\n",
    "3. Update centroids\n",
    "\n",
    "### ğŸ”¹ Interview Trap\n",
    "\n",
    "â“ *Why sensitive to initialization?*\n",
    "âœ”ï¸ Local minima\n",
    "\n",
    "\n",
    "### âœ… When to Use\n",
    "- Small dataset\n",
    "- Clear distance metric\n",
    "\n",
    "### âŒ When NOT to Use\n",
    "- Large dataset\n",
    "- High-dimensional data\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§ª Mini Project\n",
    "**Movie Recommendation System**\n",
    "\n",
    "---\n",
    "\n",
    "## 9ï¸âƒ£ Principal Component Analysis (PCA)\n",
    "\n",
    "### ğŸ”¹ Covariance Matrix\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{m}X^TX\n",
    "$$\n",
    "\n",
    "### ğŸ”¹ Eigen Decomposition\n",
    "\n",
    "$$\n",
    "\\Sigma v = \\lambda v\n",
    "$$\n",
    "\n",
    "### ğŸ”¹ Purpose\n",
    "\n",
    "* Dimensionality reduction\n",
    "\n",
    "\n",
    "### âœ… When to Use\n",
    "- High dimensional data\n",
    "- Visualization\n",
    "\n",
    "### âŒ When NOT to Use\n",
    "- Feature meaning à¦¦à¦°à¦•à¦¾à¦° à¦¹à¦²à§‡\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§ª Mini Project\n",
    "**MNIST Visualization using PCA**\n",
    "\n",
    "### ğŸ”¹ Interview Q&A\n",
    "**Q: PCA à¦•à¦¿ feature selection?**  \n",
    "âŒ à¦¨à¦¾  \n",
    "âœ… Feature transformation\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”Ÿ Neural Network (Basic)\n",
    "\n",
    "### ğŸ”¹ Neuron\n",
    "\n",
    "$$\n",
    "z = w^Tx + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "a = \\sigma(z)\n",
    "$$\n",
    "\n",
    "### ğŸ”¹ Loss\n",
    "\n",
    "$$\n",
    "L = (y - \\hat{y})^2\n",
    "$$\n",
    "\n",
    "### ğŸ”¹ Backprop Idea\n",
    "\n",
    "* Chain rule\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## ğŸ”Ÿ Gradient Boosting / XGBoost\n",
    "\n",
    "### ğŸ”¹ Interview Q&A\n",
    "**Q: Boosting à¦•à§€?**  \n",
    "ğŸ‘‰ à¦†à¦—à§‡à¦° model à¦à¦° à¦­à§à¦² à¦¥à§‡à¦•à§‡ à¦¶à§‡à¦–à§‡à¥¤\n",
    "\n",
    "---\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\frac{\\partial J}{\\partial w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "\n",
    "### ğŸ§  Intuition\n",
    "à¦à¦•à¦Ÿà¦¾à¦° à¦ªà¦° à¦à¦•à¦Ÿà¦¾ model à¦†à¦—à§‡à¦° error à¦ à¦¿à¦• à¦•à¦°à§‡à¥¤\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… When to Use\n",
    "- Tabular data\n",
    "- High performance à¦¦à¦°à¦•à¦¾à¦° à¦¹à¦²à§‡\n",
    "\n",
    "### âŒ When NOT to Use\n",
    "- Small noisy dataset\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§ª Mini Project\n",
    "**Sales Forecasting**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Final Interview Advice\n",
    "\n",
    "* Always explain **intuition first**\n",
    "* Then math\n",
    "* Then real-life example\n",
    "\n",
    "\n",
    "| Meaning | LaTeX (Rendered) |\n",
    "|--------|------------------|\n",
    "| Sum | $$\\sum$$ |\n",
    "| Fraction | $$\\frac{a}{b}$$ |\n",
    "| Square | $$x^2$$ |\n",
    "| Partial derivative | $$\\frac{\\partial}{\\partial w}$$ |\n",
    "| Theta | $$\\theta$$ |\n",
    "| Mean | $$\\mu$$ |\n",
    "| Sigma | $$\\sigma$$ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0293d31d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
