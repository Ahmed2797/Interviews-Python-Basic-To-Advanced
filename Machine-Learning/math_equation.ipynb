{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829dae1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39637d54",
   "metadata": {},
   "source": [
    "# ðŸ“ Mathâ€‘Only Interview Notes (ML + Statistics)\n",
    "\n",
    "> **Pure equations. Zero theory. Interview whiteboard ready.**\n",
    "> Format: **Real textbook LaTeX math**\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ Basic Statistics\n",
    "\n",
    "### Mean\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "### Variance\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "### Standard Deviation\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ Probability\n",
    "\n",
    "### Conditional Probability\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "### Bayes Theorem\n",
    "\n",
    "$$\n",
    "P(y|x) = \\frac{P(x|y)P(y)}{P(x)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Linear Regression\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "$$\n",
    "\\hat{y} = wx + b\n",
    "$$\n",
    "\n",
    "### Cost Function (MSE)\n",
    "\n",
    "$$\n",
    "J(w,b) = \\frac{1}{2m}\\sum_{i=1}^{m}(\\hat{y}_i - y_i)^2\n",
    "$$\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y}_i - y_i)x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y}_i - y_i)\n",
    "$$\n",
    "\n",
    "### Normal Equation\n",
    "\n",
    "$$\n",
    "\\theta = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Logistic Regression\n",
    "\n",
    "### Sigmoid Function\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "### Prediction\n",
    "\n",
    "$$\n",
    "P(y=1|x) = \\sigma(w^Tx + b)\n",
    "$$\n",
    "\n",
    "### Log Loss\n",
    "\n",
    "$$\n",
    "J(w) = -\\frac{1}{m}\\sum_{i=1}^{m}[y_i\\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£ Support Vector Machine (SVM)\n",
    "\n",
    "### Decision Boundary\n",
    "\n",
    "$$\n",
    "w^Tx + b = 0\n",
    "$$\n",
    "\n",
    "### Optimization Objective\n",
    "\n",
    "$$\n",
    "\\min \\frac{1}{2}||w||^2\n",
    "$$\n",
    "\n",
    "### Constraint\n",
    "\n",
    "$$\n",
    "y_i(w^Tx_i + b) \\ge 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£ Kâ€‘Nearest Neighbors (KNN)\n",
    "\n",
    "### Euclidean Distance\n",
    "\n",
    "$$\n",
    "d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 7ï¸âƒ£ Decision Tree\n",
    "\n",
    "### Entropy\n",
    "\n",
    "$$\n",
    "H(S) = -\\sum p_i\\log_2 p_i\n",
    "$$\n",
    "\n",
    "### Information Gain\n",
    "\n",
    "$$\n",
    "IG = H(parent) - \\sum \\frac{n_i}{n}H(child)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 8ï¸âƒ£ Random Forest\n",
    "\n",
    "### Bagging Prediction\n",
    "\n",
    "$$\n",
    "\\hat{y} = mode(\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_n)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 9ï¸âƒ£ Kâ€‘Means Clustering\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "$$\n",
    "J = \\sum_{i=1}^{k}\\sum_{x \\in C_i}||x - \\mu_i||^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”Ÿ Principal Component Analysis (PCA)\n",
    "\n",
    "### Covariance Matrix\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{m}X^TX\n",
    "$$\n",
    "\n",
    "### Eigen Equation\n",
    "\n",
    "$$\n",
    "\\Sigma v = \\lambda v\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£1ï¸âƒ£ Neural Network\n",
    "\n",
    "### Neuron\n",
    "\n",
    "$$\n",
    "z = w^Tx + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "a = \\sigma(z)\n",
    "$$\n",
    "\n",
    "### Loss (MSE)\n",
    "\n",
    "$$\n",
    "L = (y - \\hat{y})^2\n",
    "$$\n",
    "\n",
    "### Backprop (Chain Rule)\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a}\\cdot \\frac{\\partial a}{\\partial z}\\cdot \\frac{\\partial z}{\\partial w}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Interview Usage\n",
    "\n",
    "* Write equations first\n",
    "* Explain intuition verbally\n",
    "* No memorizationâ€”derive if asked\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“˜ **This document is optimized for GitHub, VS Code (with math preview), Notion, and PDF export**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688bc1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff4ef7fc",
   "metadata": {},
   "source": [
    "# ðŸ“ Machine Learning & Statistics Equations â€” Deep Explanation Notes\n",
    "\n",
    "> **Goal:** Interview-ready understanding of equations\n",
    "> Style: **Equation âžœ intuition âžœ term-by-term explanation âžœ interview angle**\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ Mean (Average)\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "### Explanation (Bangla)\n",
    "\n",
    "* à¦à¦–à¦¾à¦¨à§‡ **Î¼ (mu)** à¦¹à¦²à§‹ population mean\n",
    "* **n** = total observations à¦¸à¦‚à¦–à§à¦¯à¦¾\n",
    "* **Î£ (sum)** à¦®à¦¾à¦¨à§‡ à¦¸à¦¬ value à¦¯à§‹à¦— à¦•à¦°à¦¾\n",
    "* à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ observation à¦¯à§‹à¦— à¦•à¦°à§‡ total à¦•à§‡ **n à¦¦à¦¿à§Ÿà§‡ à¦­à¦¾à¦—** à¦•à¦°à¦²à§‡ average à¦ªà¦¾à¦“à§Ÿà¦¾ à¦¯à¦¾à§Ÿ\n",
    "\n",
    "### Intuition\n",
    "\n",
    "à¦¸à¦¬ data point à¦•à§‡ à¦¸à¦®à¦¾à¦¨ à¦—à§à¦°à§à¦¤à§à¦¬ à¦¦à¦¿à§Ÿà§‡ à¦•à§‡à¦¨à§à¦¦à§à¦°à§‡à¦° à¦®à¦¾à¦¨ à¦¬à§‡à¦° à¦•à¦°à¦¾à¥¤\n",
    "\n",
    "### Interview Tip\n",
    "\n",
    "ðŸ‘‰ *Mean sensitive to outliers* â€” à¦à¦•à¦Ÿà¦¿ extreme value mean à¦•à§‡ à¦…à¦¨à§‡à¦• à¦¸à¦°à¦¿à§Ÿà§‡ à¦¦à¦¿à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¥¤\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ Variance\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "### Explanation\n",
    "\n",
    "* **xáµ¢ âˆ’ Î¼** = à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ value mean à¦¥à§‡à¦•à§‡ à¦•à¦¤ à¦¦à§‚à¦°à§‡\n",
    "* Square à¦•à¦°à¦¾ à¦¹à§Ÿ à¦•à¦¾à¦°à¦£:\n",
    "\n",
    "  * Negative à¦¦à§‚à¦° à¦•à¦°à¦¤à§‡\n",
    "  * à¦¬à§œ deviation à¦¬à§‡à¦¶à¦¿ à¦—à§à¦°à§à¦¤à§à¦¬ à¦ªà¦¾à§Ÿ\n",
    "* à¦¸à¦¬ deviation à¦à¦° average = variance\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Data à¦•à¦¤à¦Ÿà¦¾ **spread out** à¦¤à¦¾ à¦®à¦¾à¦ªà¦¾à¥¤\n",
    "\n",
    "### Interview Trap\n",
    "\n",
    "â“ Why square, not absolute value?\n",
    "âœ”ï¸ Differentiation à¦¸à¦¹à¦œ à¦¹à§Ÿ (ML optimization à¦ à¦¦à¦°à¦•à¦¾à¦°)\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Standard Deviation\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2}\n",
    "$$\n",
    "\n",
    "### Explanation\n",
    "\n",
    "* Variance à¦à¦° square root\n",
    "* Unit à¦†à¦¬à¦¾à¦° original data à¦à¦° à¦®à¦¤à§‹ à¦¹à§Ÿ\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Average deviation from mean\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Conditional Probability\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "### Explanation\n",
    "\n",
    "* B à¦˜à¦Ÿà§‡à¦›à§‡ à¦§à¦°à§‡ à¦¨à¦¿à§Ÿà§‡ A à¦˜à¦Ÿà¦¾à¦° à¦¸à¦®à§à¦­à¦¾à¦¬à¦¨à¦¾\n",
    "* Sample space à¦›à§‹à¦Ÿ à¦¹à§Ÿà§‡ à¦¯à¦¾à§Ÿ\n",
    "\n",
    "### Real-life Example\n",
    "\n",
    "B = Student is admitted\n",
    "A = Student passed math\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£ Bayes Theorem\n",
    "\n",
    "$$\n",
    "P(y|x) = \\frac{P(x|y)P(y)}{P(x)}\n",
    "$$\n",
    "\n",
    "### Explanation\n",
    "\n",
    "* **Prior**: P(y)\n",
    "* **Likelihood**: P(x|y)\n",
    "* **Posterior**: P(y|x)\n",
    "\n",
    "### Intuition\n",
    "\n",
    "à¦ªà§à¦°à¦¨à§‹ à¦¬à¦¿à¦¶à§à¦¬à¦¾à¦¸ + à¦¨à¦¤à§à¦¨ à¦¤à¦¥à§à¦¯ = updated belief\n",
    "\n",
    "### Interview Gold Line\n",
    "\n",
    "ðŸ‘‰ *Bayes updates belief using evidence*\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£ Linear Regression Hypothesis\n",
    "\n",
    "$$\n",
    "\\hat{y} = wx + b\n",
    "$$\n",
    "\n",
    "### Explanation\n",
    "\n",
    "* **w** = slope (x à¦¬à¦¾à§œà¦²à§‡ y à¦•à¦¤à¦Ÿà¦¾ à¦¬à¦¾à§œà¦¬à§‡)\n",
    "* **b** = intercept (x=0 à¦¹à¦²à§‡ y)\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Best fit straight line\n",
    "\n",
    "---\n",
    "\n",
    "## 7ï¸âƒ£ Linear Regression Cost Function (MSE)\n",
    "\n",
    "$$\n",
    "J(w,b) = \\frac{1}{2m}\\sum_{i=1}^{m}(\\hat{y}_i - y_i)^2\n",
    "$$\n",
    "\n",
    "### Explanation\n",
    "\n",
    "* Prediction error square à¦•à¦°à¦¾ à¦¹à§Ÿ\n",
    "* **1/2** derivative à¦¸à¦¹à¦œ à¦•à¦°à¦¾à¦° à¦œà¦¨à§à¦¯\n",
    "* **m** = total samples\n",
    "\n",
    "### Interview Trap\n",
    "\n",
    "â“ Why squared error?\n",
    "âœ”ï¸ Convex + differentiable\n",
    "\n",
    "---\n",
    "\n",
    "## 8ï¸âƒ£ Gradient Descent\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\frac{\\partial J}{\\partial w}\n",
    "$$\n",
    "\n",
    "### Explanation\n",
    "\n",
    "* **Î± (learning rate)** = step size\n",
    "* Gradient direction = steepest increase\n",
    "* Minus sign â†’ error à¦•à¦®à¦¾à¦¨à§‹à¦° à¦¦à¦¿à¦•à§‡ à¦¯à¦¾à¦“à§Ÿà¦¾\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Hill à¦¥à§‡à¦•à§‡ à¦¨à¦¿à¦šà§‡ à¦¨à¦¾à¦®à¦¾\n",
    "\n",
    "---\n",
    "\n",
    "## 9ï¸âƒ£ Logistic Regression Sigmoid\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "### Explanation\n",
    "\n",
    "* Output range: (0,1)\n",
    "* Probability à¦¹à¦¿à¦¸à§‡à¦¬à§‡ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¾ à¦¯à¦¾à§Ÿ\n",
    "\n",
    "### Interview Tip\n",
    "\n",
    "Sigmoid smooth & differentiable\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”Ÿ Log Loss\n",
    "\n",
    "$$\n",
    "L = -[y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})]\n",
    "$$\n",
    "\n",
    "### Explanation\n",
    "\n",
    "* Wrong confident prediction à¦•à§‡ à¦¬à§‡à¦¶à¦¿ penalty à¦¦à§‡à§Ÿ\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£1ï¸âƒ£ SVM Margin\n",
    "\n",
    "$$\n",
    "\\min \\frac{1}{2}||w||^2\n",
    "$$\n",
    "\n",
    "### Explanation\n",
    "\n",
    "* ||w|| à¦›à§‹à¦Ÿ à¦®à¦¾à¦¨à§‡ margin à¦¬à§œ\n",
    "* Large margin â†’ better generalization\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£2ï¸âƒ£ Entropy (Decision Tree)\n",
    "\n",
    "$$\n",
    "H = -\\sum p_i \\log_2 p_i\n",
    "$$\n",
    "\n",
    "### Explanation\n",
    "\n",
    "* Purity measure\n",
    "* Pure node â†’ entropy = 0\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£3ï¸âƒ£ K-Means Objective\n",
    "\n",
    "$$\n",
    "J = \\sum_{i=1}^{k}\\sum_{x \\in C_i}||x - \\mu_i||^2\n",
    "$$\n",
    "\n",
    "### Explanation\n",
    "\n",
    "* Cluster à¦à¦° à¦­à¦¿à¦¤à¦°à§‡à¦° distance minimize à¦•à¦°à¦¾\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£4ï¸âƒ£ PCA Eigen Equation\n",
    "\n",
    "$$\n",
    "\\Sigma v = \\lambda v\n",
    "$$\n",
    "\n",
    "### Explanation\n",
    "\n",
    "* **v** = principal direction\n",
    "* **Î»** = variance amount\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£5ï¸âƒ£ Neural Network Backprop\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a}\\cdot \\frac{\\partial a}{\\partial z}\\cdot \\frac{\\partial z}{\\partial w}\n",
    "$$\n",
    "\n",
    "### Explanation\n",
    "\n",
    "* Chain rule à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦°\n",
    "* Error backward propagate à¦¹à§Ÿ\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Final Interview Strategy\n",
    "\n",
    "1. Equation à¦²à¦¿à¦–à§‹\n",
    "2. à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ symbol explain à¦•à¦°à§‹\n",
    "3. Intuition à¦¬à¦²à§‹\n",
    "4. Real-life example à¦¦à¦¾à¦“\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“˜ **This document is ideal for ML interviews, viva, and whiteboard rounds**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f372e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47bdf0b3",
   "metadata": {},
   "source": [
    "# ðŸ“ ML & Statistics Equations â€” Worked Math Examples\n",
    "\n",
    "> **Equation âžœ numeric example âžœ calculation âžœ result**\n",
    "> Goal: *You can solve on a whiteboard without fear*\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ Mean (Average)\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "Data: 2, 4, 6, 8\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{4}(2+4+6+8) = \\frac{20}{4} = 5\n",
    "$$\n",
    "\n",
    "ðŸ‘‰ Mean = **5**\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ Variance\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "Data: 2, 4, 6, 8\n",
    "Mean = 5\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\frac{1}{4}[(2-5)^2 + (4-5)^2 + (6-5)^2 + (8-5)^2]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{4}[9+1+1+9] = \\frac{20}{4} = 5\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Standard Deviation\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{\\sigma^2}\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{5} \\approx 2.24\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Conditional Probability\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "* 100 students\n",
    "* 40 passed math (B)\n",
    "* 20 passed math & physics (A âˆ© B)\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{20}{40} = 0.5\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£ Bayes Theorem\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "P(y|x) = \\frac{P(x|y)P(y)}{P(x)}\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "* Disease rate: P(y)=0.01\n",
    "* Test accuracy: P(x|y)=0.9\n",
    "* Positive test rate: P(x)=0.1\n",
    "\n",
    "$$\n",
    "P(y|x) = \\frac{0.9 \\times 0.01}{0.1} = 0.09\n",
    "$$\n",
    "\n",
    "ðŸ‘‰ Only **9%** chance of disease\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£ Linear Regression Prediction\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "\\hat{y} = wx + b\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "* w = 2\n",
    "* b = 1\n",
    "* x = 3\n",
    "\n",
    "$$\n",
    "\\hat{y} = 2(3) + 1 = 7\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 7ï¸âƒ£ Mean Squared Error (MSE)\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{m}\\sum(\\hat{y} - y)^2\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "True: [3,5]\n",
    "Predicted: [2,6]\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{2}[(2-3)^2 + (6-5)^2]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{2}[1+1] = 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 8ï¸âƒ£ Gradient Descent (Weight Update)\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\frac{\\partial J}{\\partial w}\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "* w = 5\n",
    "* learning rate Î± = 0.1\n",
    "* gradient = 4\n",
    "\n",
    "$$\n",
    "w = 5 - 0.1(4) = 4.6\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 9ï¸âƒ£ Sigmoid Function\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "z = 0\n",
    "\n",
    "$$\n",
    "\\sigma(0) = \\frac{1}{1+1} = 0.5\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”Ÿ Log Loss\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "L = -[y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})]\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "* y = 1\n",
    "* Å· = 0.8\n",
    "\n",
    "$$\n",
    "L = -\\log(0.8) \\approx 0.223\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£1ï¸âƒ£ Euclidean Distance (KNN)\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "d = \\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "Points: (1,2) and (4,6)\n",
    "\n",
    "$$\n",
    "d = \\sqrt{(3)^2 + (4)^2} = 5\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£2ï¸âƒ£ Entropy\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "H = -\\sum p_i \\log_2 p_i\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "Class split: 50% / 50%\n",
    "\n",
    "$$\n",
    "H = -(0.5\\log_2 0.5 + 0.5\\log_2 0.5) = 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£3ï¸âƒ£ K-Means Objective\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "J = \\sum ||x - \\mu||^2\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "Point x=4, centroid Î¼=2\n",
    "\n",
    "$$\n",
    "J = (4-2)^2 = 4\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£4ï¸âƒ£ PCA Eigen Example\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "\\Sigma v = \\lambda v\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "If Î» = 5 â†’ direction explains high variance\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£5ï¸âƒ£ Backpropagation\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a}\\cdot \\frac{\\partial a}{\\partial z}\\cdot \\frac{\\partial z}{\\partial w}\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "If:\n",
    "\n",
    "* âˆ‚L/âˆ‚a = 2\n",
    "* âˆ‚a/âˆ‚z = 0.5\n",
    "* âˆ‚z/âˆ‚w = 3\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = 2 \\times 0.5 \\times 3 = 3\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Interview Rule\n",
    "\n",
    "> **If you can solve numbers, you understand the math.**\n",
    "\n",
    "ðŸ“˜ This file is perfect for **practice + revision before interviews**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc17c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "668deb13",
   "metadata": {},
   "source": [
    "# ðŸ§‘â€ðŸ’» ML & Statistics Equations â€” Python Code Examples\n",
    "\n",
    "> **Equation âžœ Math âžœ Python code âžœ Output intuition**\n",
    "> Goal: *Run in VS Code / Jupyter and SEE the math working*\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ Mean (Average)\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "### Python Code\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([2, 4, 6, 8])\n",
    "mean = np.sum(data) / len(data)\n",
    "print(mean)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ Variance\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "### Python Code\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([2, 4, 6, 8])\n",
    "mean = np.mean(data)\n",
    "variance = np.sum((data - mean) ** 2) / len(data)\n",
    "print(variance)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Standard Deviation\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{\\sigma^2}\n",
    "$$\n",
    "\n",
    "### Python Code\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([2, 4, 6, 8])\n",
    "std = np.sqrt(np.mean((data - np.mean(data)) ** 2))\n",
    "print(std)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Conditional Probability\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "### Python Code\n",
    "\n",
    "```python\n",
    "passed_math = 40\n",
    "passed_both = 20\n",
    "\n",
    "prob = passed_both / passed_math\n",
    "print(prob)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£ Bayes Theorem\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "P(y|x) = \\frac{P(x|y)P(y)}{P(x)}\n",
    "$$\n",
    "\n",
    "### Python Code\n",
    "\n",
    "```python\n",
    "P_y = 0.01      # disease rate\n",
    "P_x_given_y = 0.9\n",
    "P_x = 0.1\n",
    "\n",
    "posterior = (P_x_given_y * P_y) / P_x\n",
    "print(posterior)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£ Linear Regression Prediction\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "\\hat{y} = wx + b\n",
    "$$\n",
    "\n",
    "### Python Code\n",
    "\n",
    "```python\n",
    "w = 2\n",
    "b = 1\n",
    "x = 3\n",
    "\n",
    "y_hat = w * x + b\n",
    "print(y_hat)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7ï¸âƒ£ Mean Squared Error (MSE)\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{m}\\sum(\\hat{y} - y)^2\n",
    "$$\n",
    "\n",
    "### Python Code\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array([3, 5])\n",
    "y_pred = np.array([2, 6])\n",
    "\n",
    "mse = np.mean((y_pred - y_true) ** 2)\n",
    "print(mse)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8ï¸âƒ£ Gradient Descent (Single Step)\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\frac{\\partial J}{\\partial w}\n",
    "$$\n",
    "\n",
    "### Python Code\n",
    "\n",
    "```python\n",
    "w = 5\n",
    "alpha = 0.1\n",
    "gradient = 4\n",
    "\n",
    "w_new = w - alpha * gradient\n",
    "print(w_new)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9ï¸âƒ£ Sigmoid Function\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "### Python Code\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "print(sigmoid(0))\n",
    "print(sigmoid(2))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”Ÿ Log Loss\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "L = -[y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})]\n",
    "$$\n",
    "\n",
    "### Python Code\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "y = 1\n",
    "y_hat = 0.8\n",
    "\n",
    "loss = -(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "print(loss)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£1ï¸âƒ£ Euclidean Distance (KNN)\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "d = \\sqrt{\\sum(x_i - y_i)^2}\n",
    "$$\n",
    "\n",
    "### Python Code\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "p1 = np.array([1, 2])\n",
    "p2 = np.array([4, 6])\n",
    "\n",
    "distance = np.sqrt(np.sum((p1 - p2) ** 2))\n",
    "print(distance)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£2ï¸âƒ£ Entropy\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "H = -\\sum p_i \\log_2 p_i\n",
    "$$\n",
    "\n",
    "### Python Code\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "p = np.array([0.5, 0.5])\n",
    "entropy = -np.sum(p * np.log2(p))\n",
    "print(entropy)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£3ï¸âƒ£ K-Means Objective\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "J = ||x - \\mu||^2\n",
    "$$\n",
    "\n",
    "### Python Code\n",
    "\n",
    "```python\n",
    "x = 4\n",
    "mu = 2\n",
    "\n",
    "loss = (x - mu) ** 2\n",
    "print(loss)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£4ï¸âƒ£ PCA (Covariance Matrix)\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{m}X^TX\n",
    "$$\n",
    "\n",
    "### Python Code\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[2, 3], [4, 5], [6, 7]])\n",
    "X_centered = X - np.mean(X, axis=0)\n",
    "\n",
    "cov = np.dot(X_centered.T, X_centered) / len(X)\n",
    "print(cov)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£5ï¸âƒ£ Backpropagation (Chain Rule)\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}\n",
    "$$\n",
    "\n",
    "### Python Code\n",
    "\n",
    "```python\n",
    "dL_da = 2\n",
    "da_dz = 0.5\n",
    "dz_dw = 3\n",
    "\n",
    "dL_dw = dL_da * da_dz * dz_dw\n",
    "print(dL_dw)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ How to Use This\n",
    "\n",
    "* Run each cell\n",
    "* Change numbers\n",
    "* Observe output change\n",
    "\n",
    "> **If you can code the math, you truly understand it.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0649374",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
