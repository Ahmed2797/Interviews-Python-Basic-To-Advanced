{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf26ed47",
   "metadata": {},
   "source": [
    "# üìä Statistics ‚Äì Interview Questions (MCQ + Theory + Coding) PDF Notes\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Multiple Choice Questions (MCQ)\n",
    "\n",
    "**Q1:** Which measure of central tendency is least affected by outliers?\n",
    "\n",
    "* A) Mean\n",
    "* B) Median ‚úÖ\n",
    "* C) Mode\n",
    "* D) Standard Deviation\n",
    "\n",
    "**Q2:** If a dataset is normally distributed, what percentage of data lies within 1 standard deviation from the mean?\n",
    "\n",
    "* A) 50%\n",
    "* B) 68% ‚úÖ\n",
    "* C) 95%\n",
    "* D) 99%\n",
    "\n",
    "**Q3:** What type of variable is 'Blood Group'?\n",
    "\n",
    "* A) Ordinal\n",
    "* B) Nominal ‚úÖ\n",
    "* C) Discrete\n",
    "* D) Continuous\n",
    "\n",
    "**Q4:** What is the correct formula for sample variance?\n",
    "\n",
    "* A) Œ£(x ‚àí Œº)¬≤ / N\n",
    "* B) Œ£(x ‚àí xÃÑ)¬≤ / (n ‚àí 1) ‚úÖ\n",
    "* C) Œ£(x ‚àí xÃÑ)¬≤ / n\n",
    "* D) Œ£(x ‚àí Œº)¬≤ / (N ‚àí 1)\n",
    "\n",
    "**Q5:** Which sampling technique divides the population into strata and samples from each group?\n",
    "\n",
    "* A) Simple Random Sampling\n",
    "* B) Systematic Sampling\n",
    "* C) Stratified Sampling ‚úÖ\n",
    "* D) Cluster Sampling\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Theory Questions\n",
    "\n",
    "**Q1:** Explain the difference between Descriptive and Inferential Statistics.\n",
    "\n",
    "* Descriptive: Summarizes and describes data.\n",
    "* Inferential: Draws conclusions or predictions about a population from a sample.\n",
    "\n",
    "**Q2:** Define population and sample with examples.\n",
    "\n",
    "* Population: All students in a university.\n",
    "* Sample: 200 students selected from the university.\n",
    "\n",
    "**Q3:** What is variance and why is it important?\n",
    "\n",
    "* Variance measures data spread from the mean.\n",
    "* Important to detect variability, assess risk, and understand data distribution.\n",
    "\n",
    "**Q4:** Describe types of variables with examples.\n",
    "\n",
    "* Qualitative: Nominal (Gender), Ordinal (Ratings)\n",
    "* Quantitative: Discrete (Number of students), Continuous (Height)\n",
    "\n",
    "**Q5:** Explain the difference between Z-test and T-test.\n",
    "\n",
    "* Z-test: Large sample, population SD known\n",
    "* T-test: Small sample, population SD unknown\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Coding Questions (Python)\n",
    "\n",
    "**Q1:** Compute Mean, Median, Mode, Variance, and Standard Deviation.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from statistics import mode\n",
    "\n",
    "data = [10, 20, 20, 30, 40]\n",
    "print(\"Mean:\", np.mean(data))\n",
    "print(\"Median:\", np.median(data))\n",
    "print(\"Mode:\", mode(data))\n",
    "print(\"Variance:\", np.var(data, ddof=1))  # Sample variance\n",
    "print(\"Std Dev:\", np.std(data, ddof=1))\n",
    "```\n",
    "\n",
    "**Q2:** Create a stratified sample from a dataset.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Department': ['CS','CS','EE','EE','ME','ME'],\n",
    "    'Score': [90,85,88,92,75,80]\n",
    "})\n",
    "\n",
    "# Stratified sampling\n",
    "train, test = train_test_split(data, test_size=0.5, stratify=data['Department'], random_state=42)\n",
    "print(test)\n",
    "```\n",
    "\n",
    "**Q3:** Calculate Z-score of a value.\n",
    "\n",
    "```python\n",
    "value = 70\n",
    "mean = np.mean(data['Score'])\n",
    "std = np.std(data['Score'], ddof=1)\n",
    "z_score = (value - mean) / std\n",
    "print(z_score)\n",
    "```\n",
    "\n",
    "**Q4:** Simulate a normal distribution and plot histogram.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.random.normal(50, 10, 1000)\n",
    "plt.hist(data, bins=30, color='skyblue')\n",
    "plt.title('Normal Distribution')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Q5:** Compute correlation between two variables.\n",
    "\n",
    "```python\n",
    "x = [1,2,3,4,5]\n",
    "y = [2,4,6,8,10]\n",
    "correlation = np.corrcoef(x, y)[0,1]\n",
    "print(\"Correlation:\", correlation)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **End of Statistics Interview Questions (MCQ + Theory + Coding)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57803a97",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044bd13c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e7473d3",
   "metadata": {},
   "source": [
    "# ü§ñ ML & Data Science Interview Questions\n",
    "\n",
    "## 1. Linear Regression\n",
    "\n",
    "**Question:** Explain Linear Regression and how it works.\n",
    "\n",
    "**Answer:**  \n",
    "Linear Regression predicts a continuous output \\(y\\) from input \\(X\\) using a linear relationship:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\epsilon\n",
    "$$\n",
    "\n",
    "- \\(\\beta_i\\) = coefficients (weights)  \n",
    "- \\(\\epsilon\\) = error term  \n",
    "\n",
    "**Example:**\n",
    "\n",
    "Predict house price based on `size`:\n",
    "\n",
    "Data:  \n",
    "| Size (sq.ft) | Price ($k) |\n",
    "|--------------|------------|\n",
    "| 1000         | 200        |\n",
    "| 1500         | 250        |\n",
    "\n",
    "Fit linear model ‚Üí predict new house prices.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Logistic Regression\n",
    "\n",
    "**Question:** Difference between Linear and Logistic Regression.\n",
    "\n",
    "**Answer:**  \n",
    "- Logistic regression predicts **probabilities** (0-1) for classification.  \n",
    "- Uses **sigmoid function**:\n",
    "\n",
    "$$\n",
    "p = \\frac{1}{1 + e^{-z}}, \\quad z = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n\n",
    "$$\n",
    "\n",
    "**Example:** Predict if a student passes (1) or fails (0) based on hours studied.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Decision Tree\n",
    "\n",
    "**Question:** How does a Decision Tree decide splits?\n",
    "\n",
    "**Answer:**  \n",
    "- Uses **metrics like Gini Index, Entropy** to choose the best split.  \n",
    "\n",
    "**Entropy Formula:**\n",
    "\n",
    "$$\n",
    "Entropy(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "- \\(p_i\\) = proportion of class \\(i\\) in the set  \n",
    "- \\(c\\) = number of classes  \n",
    "\n",
    "**Example:** Classify whether a fruit is apple/orange based on color and size.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. SVM (Support Vector Machine)\n",
    "\n",
    "**Question:** How does SVM work?\n",
    "\n",
    "**Answer:**  \n",
    "- SVM finds a **hyperplane** that separates classes with maximum margin.  \n",
    "\n",
    "**Equation of Hyperplane:**\n",
    "\n",
    "$$\n",
    "w \\cdot x + b = 0\n",
    "$$\n",
    "\n",
    "- \\(w\\) = weight vector  \n",
    "- \\(b\\) = bias  \n",
    "\n",
    "**Kernel Trick:** Allows non-linear separation using transformations like RBF, polynomial.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. K-Nearest Neighbors (KNN)\n",
    "\n",
    "**Question:** Explain KNN and distance metrics.\n",
    "\n",
    "**Answer:**  \n",
    "- KNN classifies based on **majority vote of k nearest neighbors**.  \n",
    "- Common distance metrics:\n",
    "\n",
    "$$\n",
    "\\text{Euclidean: } d = \\sqrt{\\sum (x_i - y_i)^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Manhattan: } d = \\sum |x_i - y_i|\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Bias vs Variance\n",
    "\n",
    "**Question:** What is Bias-Variance tradeoff?\n",
    "\n",
    "**Answer:**  \n",
    "- **Bias:** Error due to oversimplified model (underfitting)  \n",
    "- **Variance:** Error due to over-complex model (overfitting)  \n",
    "\n",
    "**Total Error:**\n",
    "\n",
    "$$\n",
    "\\text{Total Error} = Bias^2 + Variance + \\text{Irreducible Error}\n",
    "$$\n",
    "\n",
    "**Example:** Linear model (high bias) vs deep tree (high variance).\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Cross Validation\n",
    "\n",
    "**Question:** How to validate ML models?\n",
    "\n",
    "**Answer:**  \n",
    "- Split data into **k folds**. Train on k-1 folds, test on 1 fold. Repeat k times.  \n",
    "- Reduces overfitting and improves generalization.\n",
    "\n",
    "**Example:** 5-Fold CV: data split into 5 parts ‚Üí train on 4, test on 1, rotate.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. PCA (Principal Component Analysis)\n",
    "\n",
    "**Question:** What is PCA and why use it?\n",
    "\n",
    "**Answer:**  \n",
    "- PCA reduces **dimensionality** by creating new uncorrelated features (principal components).  \n",
    "- Maximize **variance** along each component.  \n",
    "\n",
    "**Math:**\n",
    "\n",
    "$$\n",
    "Z = X W\n",
    "$$\n",
    "\n",
    "- \\(X\\) = original data matrix  \n",
    "- \\(W\\) = eigenvectors of covariance matrix  \n",
    "\n",
    "**Example:** Reduce 50 features to 10 principal components for faster ML training.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Confusion Matrix & Metrics\n",
    "\n",
    "**Question:** Explain Confusion Matrix and evaluation metrics.\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "| Predicted | Actual |        |\n",
    "|-----------|--------|--------|\n",
    "| TP        | FP     |        |\n",
    "| FN        | TN     |        |\n",
    "\n",
    "- **Accuracy:** \\( \\frac{TP+TN}{TP+FP+FN+TN} \\)  \n",
    "- **Precision:** \\( \\frac{TP}{TP+FP} \\)  \n",
    "- **Recall:** \\( \\frac{TP}{TP+FN} \\)  \n",
    "- **F1-Score:** \\( 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall} \\)\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Regularization\n",
    "\n",
    "**Question:** Difference between L1 and L2 regularization.\n",
    "\n",
    "**Answer:**  \n",
    "- **L1 (Lasso):** Adds absolute weights penalty ‚Üí sparsity, feature selection\n",
    "\n",
    "$$\n",
    "Loss = MSE + \\lambda \\sum |w_i|\n",
    "$$\n",
    "\n",
    "- **L2 (Ridge):** Adds squared weights penalty ‚Üí prevents large weights\n",
    "\n",
    "$$\n",
    "Loss = MSE + \\lambda \\sum w_i^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Tips for Interviews\n",
    "\n",
    "- Always explain **intuition + math + example**  \n",
    "- Draw diagrams when possible (e.g., Decision Tree, SVM)  \n",
    "- Use **real dataset examples** like Iris, Titanic  \n",
    "- Mention **overfitting & regularization** for complex models  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c3af78",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
